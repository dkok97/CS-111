NAME: Dinkar Khattar
EMAIL: dinkarkhattar@ucla.edu
ID: 204818138

Question 2.3.1 - Cycles in the basic list implementation:

-In 1 and 2 thread lists, since there are only a few threads, locking does not
take up many cycles. So most cycles are probably spent doing the actually operations
like insert, delete and lookup.
-These are the most expensive parts since the parts that take up time can only be
either the locking cycles, or operations on the SortedList, and in this case, as
we established, the number of threads is low, so threads don't use a lot of cycles
waiting to run, so the latter is most expensive.
-In the high-thread spin-lock tests, most of the times/cycles are spent when the
threads are waiting to run the critical sections. With the increase in number of
threads, more threads are not running, and waiting in line (spinning), and the
throughput is reduced.
-In the high-thread mutex lock tests, similar to the spin-lock tests, most cycles
are spent on the threads waiting for the mutex lock to be lifted so they can run
the critical section.

QUESTION 2.3.2 - Execution Profiling:

-We notice that the part of the code that takes up the most cycles is the fuction
lock, where we are spin-locking the thread. The line responsible for this is:
(while (__sync_lock_test_and_set(&all_lists[h].spin_lock_var, 1) == 1);)
The other operations that take up a lot of cycles are lookup and insert.
-The reason locking takes up so many cycles with a large number of threads is
because we have only one thread running the critical sections, while the other
threads are using up CPU cycles spinning on the lock, waiting for their turn.

QUESTION 2.3.3 - Mutex Wait Time:

-The average lock-wait time rises so dramatically with the number of contending
threads because out variable is proportional to how long all the threads are waiting
for. With many threads, only 1 thread is executing the critical section, while all
the other threads are waiting to execute, this increasing wait time.
-The total time per operation increases less dramatically because there are two
factors involved in this case. The time increases because the mutex_lock wait
time increases with the increase in the number of threads, since more threads
are waiting to run the critical section. The total time is balanced out by the
time taken to do the list operations. This is because the time to perform operations
on SortedList is somewhat constant, and this added to the wait time reduces the
increase in time.
-The wait time per operation goes higher than the completion time per operation
because the wait time increases with the increasing number of threads since each
thread's wait time is added to the total wait time. On the other hand, the completion
time only increases because of the operations on the list, which are somewhat constant
for each thread. So the wait time builds up due to high contention, and the completion
time only slightly increases due to mutex_locks.


QUESTION 2.3.4 - Performance of Partitioned Lists:

-For synchronized methods (spin and mutex), there is an increase in throughput, so
the performance increases. This is because we divide the shared resource (list)
into smaller resources, so the lock time is reduced. Additionally, we are dealing
with smaller lists, so the list operations don't take as long.
-The throughput does not keep increases at the same rate with the increasing number
of lists. This is because when the list resource because so small, it only contains
a few elements, and the number of lists increase. So the lock that is used before
calculating the length is waiting for a longer time since we need to add up many
lists. This reduces the increase in throughput. Also, many lists with a few elements
basically behaves like one big list.
-Yes, to some extent, it does seem reasonable to say that  the throughput of an
N-way partitioned list should be equivalent to the throughput of a single list
with fewer (1/N) threads. We have an increased throughput with a high number of
lists, and high throughput with fewer threads. We notice this in the graph, for
example, the throughput for 1 list and 2 threads is similar to the throughput
for 4 lists 16 threads.

FILES:

lab2_list.c:
contains the program that implements the multi-threaded SortedList program as specified
in the spec, updated with the --lists option to divide the shared resource

SortedList.c:
contains the program that implements the functions defined in SortedList.h

SortedList.h:
contains function definitions for operations on a doubly linked list

lab2_list.csv:
contains the statistics outputted by lab2_add and lab2_list

lab2_list.gp:
contains code that generates appropriate graphs for the data in the csv files

.png files:
the graphs generated by the two programs above

Makefile:
It contains 6 targets:

build: compile all programs
tests: run all (over 200) specified test cases to generate results in CSV files.
graphs: runs tests and uses gnuplot(1) and the supplied data reduction scripts to generate the required graphs
dist: cleans, builds, runs tests and creates the deliverable tarball
clean: delete all programs and output created by the Makefile (except *png and *csv)
profile: runs the profile tool and outputs the report

README:
The README contains descriptions for all the files and the sources
I used.

Source(s):

https://github.com/google/pprof
